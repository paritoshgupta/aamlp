{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor flow version --> 2.3.0\n",
      "Keras version --> 2.4.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install efficientnet\n",
    "# !pip install pydot\n",
    "# !pip install graphviz\n",
    "# !pip install pydotplus\n",
    "import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import math\n",
    "from PIL import Image\n",
    "# import pydot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow.keras.backend as K\n",
    "# import pydotplus\n",
    "# from pydotplus import graphviz\n",
    "# import pydot\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.utils.data_utils import Sequence\n",
    "# import efficientnet.tfkeras as efn\n",
    "from keras.applications import VGG16\n",
    "tf.keras.backend.clear_session()\n",
    "# BytesList = tf.train.BytesList\n",
    "# FloatList = tf.train.FloatList\n",
    "# Int64List = tf.train.Int64List\n",
    "# Feature = tf.train.Feature\n",
    "# Features = tf.train.Features\n",
    "# Example = tf.train.Example\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "# np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "\n",
    "print(f\"Tensor flow version --> {tf.__version__}\")\n",
    "# from tensorflow.keras.utils import Sequence\n",
    "print(f\"Keras version --> {keras.__version__}\")\n",
    "\n",
    "# # detect and init the TPU\n",
    "# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "# tf.config.experimental_connect_to_cluster(tpu)\n",
    "# tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "# # instantiate a distribution strategy\n",
    "# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path parameters\n",
    "OUTPUT_PATH = \"../input/osiccsvfiles/osic_csv_files/\"\n",
    "OUTPUT_PATH_LUNG_MASK_IMAGES_1MM = \"../input/osic-ads-images/lung_mask_1mm/\"\n",
    "\n",
    "# augmentation parameters\n",
    "ROT_ = 0\n",
    "SHR_ = 2\n",
    "HZOOM_ = 4\n",
    "WZOOM_ = 4\n",
    "HSHIFT_ = 4\n",
    "WSHIFT_ = 4\n",
    "\n",
    "\n",
    "# ROT_ = 0\n",
    "# SHR_ = 2.0\n",
    "# HZOOM_ = 8.0\n",
    "# WZOOM_ = 8.0\n",
    "# HSHIFT_ = 8.0\n",
    "# WSHIFT_ = 8.0\n",
    "\n",
    "# input image size\n",
    "image_size = 299\n",
    "\n",
    "# batch size\n",
    "bs = 16\n",
    "\n",
    "# FOLDS PARAMETERS\n",
    "fold = 0 # which fold to run\n",
    "n_folds = 10 # CSV FILE\n",
    "\n",
    "# FINE TUNING PARAMETERS\n",
    "# Number of layers to fine-tune of base model\n",
    "trainable = 8\n",
    "early_stopping_fine_tuning = 8\n",
    "\n",
    "# MODEL PARAMATERS AND LEARNING RATE\n",
    "EPOCHS = 100\n",
    "optimizer = keras.optimizers.Adam(lr=0.00125, beta_1=0.9, beta_2=0.999)\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=1)\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(f\"/kaggle/working/osic_pat_level_image_index_{n_folds}FOLDS_{EPOCHS}E_f_{fold}.h5\",\n",
    "                                                save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "# AUGMENTATION PARAMETER\n",
    "TTA_STEPS = 11\n",
    "\n",
    "# Test Patients\n",
    "patients_test = [\"ID00419637202311204720264\", \"ID00421637202311550012437\", \"ID00422637202311677017371\", \"ID00423637202312137826377\", \"ID00426637202313170790466\"]\n",
    "\n",
    "# Junk Patients\n",
    "junk_patients = [ \"ID00170637202238079193844\",\"ID00090637202204766623410\"] \n",
    "\n",
    "\n",
    "########################## PLAYGROUND ########################################\n",
    "# AUG_BATCH = 16\n",
    "# patients_image_perc_20 = [\"ID00168637202237852027833\",\"ID00099637202206203080121\", \n",
    "#                           \"ID00285637202278913507108\", \"ID00371637202296828615743\", \n",
    "#                           \"ID00186637202242472088675\"]\n",
    "\n",
    "# patients_image_perc_70 = [\"ID00123637202217151272140\", \"ID00108637202209619669361\",\n",
    "#                           \"ID00190637202244450116191\", \"ID00135637202224630271439\",\n",
    "#                           \"ID00020637202178344345685\", \"ID00027637202179689871102\",\n",
    "#                           \"ID00035637202182204917484\", \"ID00109637202210454292264\",\n",
    "#                           \"ID00233637202260580149633\", \"ID00089637202204675567570\",\n",
    "#                           \"ID00032637202181710233084\", \"ID00393637202302431697467\",\n",
    "#                           \"ID00075637202198610425520\", \"ID00067637202189903532242\"]\n",
    "\n",
    "# no good percentile index [0.2, 0.3, 0.7] for these 2 Patient \n",
    "#\"ID00094637202205333947361\", #\"ID00086637202203494931510\",\n",
    "#\"ID00067637202189903532242\", \"ID00014637202177757139317\", \\ # \"ID00240637202264138860065\", #  \"ID00122637202216437668965\"\n",
    "\n",
    "# print(f\"# of Patients with 20th Percentile Image as Input to pre-trained model --> {len(set(patients_image_perc_20))}\")\n",
    "# print(f\"# of Patients with 70th Percentile Image as Input to pre-trained model --> {len(set(patients_image_perc_70))}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n",
    "    # returns 3x3 transformmatrix which transforms indicies\n",
    "        \n",
    "    # CONVERT DEGREES TO RADIANS\n",
    "    rotation = math.pi * rotation / 180.\n",
    "    shear = math.pi * shear / 180.\n",
    "    \n",
    "    # ROTATION MATRIX\n",
    "    c1 = tf.math.cos(rotation)\n",
    "    s1 = tf.math.sin(rotation)\n",
    "    one = tf.constant([1],dtype='float32')\n",
    "    zero = tf.constant([0],dtype='float32')\n",
    "    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n",
    "        \n",
    "    # SHEAR MATRIX\n",
    "    c2 = tf.math.cos(shear)\n",
    "    s2 = tf.math.sin(shear)\n",
    "    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n",
    "    \n",
    "    # ZOOM MATRIX\n",
    "    zoom_matrix = tf.reshape( tf.concat([one/height_zoom,zero,zero, zero,one/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n",
    "    \n",
    "    # SHIFT MATRIX\n",
    "    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n",
    "    \n",
    "    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))\n",
    "\n",
    "\n",
    "def transform(image, image_size):\n",
    "    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n",
    "    # output - image randomly rotated, sheared, zoomed, and shifted\n",
    "    DIM = image_size[0]\n",
    "    XDIM = DIM%2 #fix for size 331\n",
    "    \n",
    "    rot = ROT_ * tf.random.normal([1], dtype='float32')\n",
    "    shr = SHR_ * tf.random.normal([1], dtype='float32') \n",
    "    h_zoom = 1.0 + tf.random.normal([1], dtype='float32') / HZOOM_\n",
    "    w_zoom = 1.0 + tf.random.normal([1], dtype='float32') / WZOOM_\n",
    "    h_shift = HSHIFT_ * tf.random.normal([1], dtype='float32') \n",
    "    w_shift = WSHIFT_ * tf.random.normal([1], dtype='float32') \n",
    "\n",
    "    # GET TRANSFORMATION MATRIX\n",
    "    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n",
    "\n",
    "    # LIST DESTINATION PIXEL INDICES\n",
    "    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\n",
    "    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\n",
    "    z = tf.ones([DIM*DIM],dtype='int32')\n",
    "    \n",
    "    idx = tf.stack([x,y,z])    \n",
    "    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n",
    "    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n",
    "    idx2 = K.cast(idx2,dtype='int32')\n",
    "    idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2)\n",
    "\n",
    "    \n",
    "    # FIND ORIGIN PIXEL VALUES           \n",
    "    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\n",
    "    d = tf.gather_nd(image,tf.transpose(idx3))\n",
    "        \n",
    "    return tf.reshape(d,[DIM,DIM,3])\n",
    "\n",
    "\n",
    "def score(y_true, y_pred):\n",
    "    C1 = tf.constant(70, dtype='float32')\n",
    "    C2 = tf.constant(1000, dtype=\"float32\")\n",
    "    tf.dtypes.cast(y_true, tf.float32)\n",
    "    tf.dtypes.cast(y_pred, tf.float32)\n",
    "    sigma = y_pred[:, 2] - y_pred[:, 0]\n",
    "    fvc_pred = y_pred[:, 1]\n",
    "    \n",
    "    #sigma_clip = sigma + C1\n",
    "    sigma_clip = tf.maximum(sigma, C1)\n",
    "    delta = tf.abs(y_true[:, 0] - fvc_pred)\n",
    "    delta = tf.minimum(delta, C2)\n",
    "    sq2 = tf.sqrt( tf.dtypes.cast(2, dtype=tf.float32) )\n",
    "    metric = (delta / sigma_clip)*sq2 + tf.math.log(sigma_clip* sq2)\n",
    "    return keras.backend.mean(metric)\n",
    "\n",
    "def qloss(y_true, y_pred):\n",
    "    # Pinball loss for multiple quantiles\n",
    "    qs = [0.2, 0.50, 0.8]\n",
    "    q = tf.constant(np.array([qs]), dtype=tf.float32)\n",
    "    e = y_true - y_pred\n",
    "    v = tf.maximum(q*e, (q-1)*e)\n",
    "    return keras.backend.mean(v)\n",
    "\n",
    "def mloss(_lambda):\n",
    "    def loss(y_true, y_pred):\n",
    "        return _lambda * qloss(y_true, y_pred) + (1 - _lambda)*score(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def convert_to_tensor(arg):\n",
    "    out = tf.convert_to_tensor(arg, dtype=tf.float32)\n",
    "    return out\n",
    "\n",
    "def plot_image(image):\n",
    "    plt.imshow(image, cmap=\"gray\", interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")    \n",
    "\n",
    "def plot_color_image(image):\n",
    "    plt.imshow(image, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def central_crop(image):\n",
    "    shape = tf.shape(image)\n",
    "    min_dim = tf.reduce_min([shape[0], shape[1]])\n",
    "    top_crop = (shape[0] - min_dim) // 4\n",
    "    bottom_crop = shape[0] - top_crop\n",
    "    left_crop = (shape[1] - min_dim) // 4\n",
    "    right_crop = shape[1] - left_crop\n",
    "    return image[top_crop:bottom_crop, left_crop:right_crop]\n",
    "\n",
    "def random_crop(image):\n",
    "    shape = tf.shape(image)\n",
    "    min_dim = tf.reduce_min([shape[0], shape[1]]) * 90 // 100\n",
    "    return tf.image.random_crop(image, [min_dim, min_dim, 3])\n",
    "\n",
    "def cutmix_mixup(image):\n",
    "    DIM = IMAGE_SIZE[0]\n",
    "    CUTMIX_PROB = 0.666\n",
    "    MIXUP_PROB = 0.666\n",
    "    # FOR SWITCH PERCENT OF TIME WE DO CUTMIX AND (1-SWITCH) WE DO MIXUP\n",
    "    image2 = cutmix(image, CUTMIX_PROB)\n",
    "    image3 = mixup(image,MIXUP_PROB)\n",
    "    imgs = []\n",
    "    for j in range(AUG_BATCH):\n",
    "        P = tf.cast( tf.random.uniform([],0,1)<=SWITCH, tf.float32)\n",
    "        imgs.append(P*image2[j,]+(1-P)*image3[j,])\n",
    "    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?)\n",
    "    image4 = tf.reshape(tf.stack(imgs),(AUG_BATCH,DIM,DIM,3))\n",
    "    return image4\n",
    "\n",
    "def cutmix(image, PROBABILITY = 1.0):\n",
    "    # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3]\n",
    "    # output - a batch of images with cutmix applied\n",
    "    DIM = IMAGE_SIZE[0]\n",
    "    \n",
    "    imgs = []\n",
    "    for j in range(AUG_BATCH):\n",
    "        # DO CUTMIX WITH PROBABILITY DEFINED ABOVE\n",
    "        P = tf.cast( tf.random.uniform([],0,1)<=PROBABILITY, tf.int32)\n",
    "        # CHOOSE RANDOM IMAGE TO CUTMIX WITH\n",
    "        k = tf.cast( tf.random.uniform([],0,AUG_BATCH),tf.int32)\n",
    "        # CHOOSE RANDOM LOCATION\n",
    "        x = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\n",
    "        y = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\n",
    "        b = tf.random.uniform([],0,1) # this is beta dist with alpha=1.0\n",
    "        WIDTH = tf.cast( DIM * tf.math.sqrt(1-b),tf.int32) * P\n",
    "        ya = tf.math.maximum(0,y-WIDTH//2)\n",
    "        yb = tf.math.minimum(DIM,y+WIDTH//2)\n",
    "        xa = tf.math.maximum(0,x-WIDTH//2)\n",
    "        xb = tf.math.minimum(DIM,x+WIDTH//2)\n",
    "        # MAKE CUTMIX IMAGE\n",
    "        one = image[j,ya:yb,0:xa,:]\n",
    "        two = image[k,ya:yb,xa:xb,:]\n",
    "        three = image[j,ya:yb,xb:DIM,:]\n",
    "        middle = tf.concat([one,two,three],axis=1)\n",
    "        img = tf.concat([image[j,0:ya,:,:],middle,image[j,yb:DIM,:,:]],axis=0)\n",
    "        imgs.append(img)\n",
    "            \n",
    "    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?)\n",
    "    image2 = tf.reshape(tf.stack(imgs),(AUG_BATCH,DIM,DIM,3))\n",
    "    return image2\n",
    "\n",
    "def mixup(image, PROBABILITY = 1.0):\n",
    "    # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3]\n",
    "    # output - a batch of images with cutmix applied\n",
    "    DIM = IMAGE_SIZE[0]\n",
    "    \n",
    "    imgs = []\n",
    "    for j in range(AUG_BATCH):\n",
    "        # DO MIXUP WITH PROBABILITY DEFINED ABOVE\n",
    "        P = tf.cast( tf.random.uniform([],0,1)<=PROBABILITY, tf.float32)\n",
    "        # CHOOSE RANDOM\n",
    "        k = tf.cast( tf.random.uniform([],0,AUG_BATCH),tf.int32)\n",
    "        a = tf.random.uniform([],0,1)*P # this is beta dist with alpha=1.0\n",
    "        # MAKE MIXUP IMAGE\n",
    "        img1 = image[j,]\n",
    "        img2 = image[k,]\n",
    "        imgs.append((1-a)*img1 + a*img2)\n",
    "            \n",
    "    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?)\n",
    "    image2 = tf.reshape(tf.stack(imgs),(AUG_BATCH,DIM,DIM,3))\n",
    "    return image2\n",
    "\n",
    "\n",
    "# def preprocess(image):\n",
    "\n",
    "# #     if aug:\n",
    "# #         image = tf.image.random_flip_left_right(image)\n",
    "#             #img = tf.image.random_hue(img, 0.01)\n",
    "# #         image = tf.image.random_saturation(image, 0.7, 1.3)\n",
    "# #         image = tf.image.random_contrast(image, 0.8, 1.2)\n",
    "# #         image = tf.image.random_brightness(image, 0.1)  \n",
    "#     resized_image = tf.image.resize(image, [image_size, image_size])\n",
    "#     final_image = keras.applications.xception.preprocess_input(resized_image*255)\n",
    "    \n",
    "#     return final_image\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    \n",
    "    def __init__(self, dataset, batch_size, aug, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.aug = aug\n",
    "        self.dataset = dataset\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(self.dataset.shape[0])\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(self.dataset.shape[0]/self.batch_size))\n",
    " \n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        # Generate indexes of the batch\n",
    "        idxs = [i for i in range(index*self.batch_size,(index+1)*self.batch_size)]\n",
    " \n",
    "        # Find list of IDs\n",
    "        ids_tmp = [self.indexes[k] for k in idxs]\n",
    "        \n",
    "        # Generate data\n",
    "        # tabular\n",
    "        tab = self.dataset.loc[ids_tmp, ['Male', 'Female', 'Ex-smoker', 'Never smoked', \n",
    "                                         'Currently smokes', 'age', 'week', 'base_fvc']].to_numpy(dtype=np.float32)\n",
    "        \n",
    "        # image\n",
    "        patient_ids = list(self.dataset.loc[ids_tmp, 'Patient'])\n",
    "        imgs_master = []\n",
    "        \n",
    "        for pat_id in patient_ids:    \n",
    "            images = os.listdir(os.path.join(OUTPUT_PATH_LUNG_MASK_IMAGES_1MM, pat_id))\n",
    "            sorted_images = sorted(images,key=lambda x: int(os.path.splitext(x)[0]))\n",
    "#             slices = [Image.open(f\"{OUTPUT_PATH_LUNG_MASK_IMAGES_1MM}/{pat_id}/{sorted_images[index]}\").convert('L') for index, file in enumerate(sorted_images)]    \n",
    "            \n",
    "            \n",
    "            ######## OLD ########\n",
    "#             img1 = np.mean([np.array(slices[i]) for i in range(int(0.15 * len(slices)), int(0.4 * len(slices)))], axis=0)/255\n",
    "#             img1 = tf.convert_to_tensor(img1.reshape((*img1.shape, 1)))\n",
    "#             img2 = np.mean([np.array(slices[i]) for i in range(int(0.4 * len(slices)/2), int(0.65 * len(slices)))], axis=0)/255\n",
    "#             img2 = tf.convert_to_tensor(img2.reshape((*img2.shape, 1)))\n",
    "#             img3 = np.mean([np.array(slices[i]) for i in range(int(0.65 * len(slices)), int(0.9 * len(slices)))], axis=0)/255\n",
    "#             img3 = tf.convert_to_tensor(img3.reshape((*img3.shape, 1)))\n",
    "\n",
    "#             randint = np.random.randint(0,3)\n",
    "#             if randint == 0:\n",
    "#                 img_to_resize = img1\n",
    "#             elif randint == 1:\n",
    "#                 img_to_resize = img2\n",
    "#             else:\n",
    "#                 img_to_resize = img3\n",
    "\n",
    "            ######## NEW Augementation ########\n",
    "            ###### All processing on GPU!!! #######\n",
    "        \n",
    "            ##### FOR TEST PATIENTS ONLY ######\n",
    "#             if pat_id in patients_test and self.aug==True:\n",
    "#                 combo = [0.2, 0.3, 0.5, 0.7, 0.8]\n",
    "#                 perc = np.random.choice(combo)\n",
    "#                 print(perc)\n",
    "#             else:\n",
    "#                 combo = [0.2, 0.3, 0.5, 0.7, 0.8]\n",
    "#                 perc = np.random.choice(combo)\n",
    "              \n",
    "############## NEW EXPERIMENT TO BE TRIED ###################             \n",
    "#             combo = [0.2, 0.3, 0.7]\n",
    "#             perc = np.random.choice(combo)\n",
    "                \n",
    "\n",
    "#             if pat_id in patients_image_perc_20:\n",
    "#                 perc = 0.2\n",
    "#             elif pat_id in patients_image_perc_70:\n",
    "#                 perc = 0.7\n",
    "#             elif pat_id in \"ID00423637202312137826377\":\n",
    "#                 print(\"yes\")\n",
    "#                 combo = [0.4, 0.3]\n",
    "#                 perc = np.random.choice(combo)\n",
    "#                 print(perc)\n",
    "#                 perc = 0.4\n",
    "#             elif pat_id in \"ID00426637202313170790466\":\n",
    "#                 print(\"yes\")\n",
    "#                 perc = 0.7\n",
    "#             elif pat_id in \"ID00421637202311550012437\":\n",
    "#                 perc = 0.2\n",
    "            \n",
    "    \n",
    "#             #################### MONTAGE ####################\n",
    "            combo = [0.2, 0.3, 0.5, 0.7, 0.8]\n",
    "            perc = np.random.choice(combo, 4, replace=False)\n",
    "            inputImages = []\n",
    "            \n",
    "            for perc_tmp in perc:\n",
    "                index = int(len(sorted_images)*perc_tmp)\n",
    "                image = tf.keras.preprocessing.image.img_to_array(Image.open(f\"{OUTPUT_PATH_LUNG_MASK_IMAGES_1MM}/{pat_id}/{sorted_images[index]}\").convert('L'))/25\n",
    "            #     print(image.shape)\n",
    "            #     image = image[:, :, 0]\n",
    "                resized_image = tf.image.resize(image, [128, 128])\n",
    "                inputImages.append(resized_image[:, :, 0])\n",
    "\n",
    "            final_image = np.zeros((256, 256))\n",
    "            final_image[0:128, 0:128] = inputImages[0]\n",
    "            final_image[0:128, 128:256] = inputImages[1]\n",
    "            final_image[128:256, 0:128] = inputImages[2]\n",
    "            final_image[128:256, 128:256] = inputImages[3]\n",
    "            image = final_image\n",
    "            ############################################################\n",
    "            \n",
    "            # SAME AS OLD PART\n",
    "            # print(image.shape)\n",
    "            img_to_resize = tf.convert_to_tensor(image.reshape((*image.shape, 1)))\n",
    "            # print(img_to_resize.shape)\n",
    "            images = tf.convert_to_tensor([img_to_resize]*3)\n",
    "            # print(images.shape)\n",
    "            images_reduced_mean = tf.reduce_mean(images, axis=3)\n",
    "            image_pre_proc = tf.stack([images_reduced_mean[0], images_reduced_mean[1], images_reduced_mean[2]], axis=-1)\n",
    "            \n",
    "            \n",
    "            ############# old ####################\n",
    "#              ##### FOR TEST PATIENTS ONLY ######\n",
    "#             if pat_id in patients_test and self.aug==True:\n",
    "#                 combo = [0.2, 0.3, 0.5, 0.7, 0.8]\n",
    "#                 perc = np.random.choice(combo)\n",
    "#                 print(perc)\n",
    "#             else:\n",
    "#                 combo = [0.2, 0.3, 0.5, 0.7, 0.8]\n",
    "#                 perc = np.random.choice(combo)\n",
    "\n",
    "#             index = int(len(sorted_images)*perc)\n",
    "#             image = tf.keras.preprocessing.image.img_to_array(Image.open(f\"{OUTPUT_PATH_LUNG_MASK_IMAGES_1MM}/{pat_id}/{sorted_images[index]}\").convert('L'))/255\n",
    "#             image = image[:, :, 0] \n",
    "#             img_to_resize = tf.convert_to_tensor(image.reshape((*image.shape, 1)))\n",
    "#             images = tf.convert_to_tensor([img_to_resize]*3)\n",
    "#             images_reduced_mean = tf.reduce_mean(images, axis=3)\n",
    "#             image_pre_proc = tf.stack([images_reduced_mean[0], images_reduced_mean[1], images_reduced_mean[2]], axis=-1)\n",
    "\n",
    "            # With Augmentation (Training time)\n",
    "            if self.aug:\n",
    "                trans_image = transform(image_pre_proc, image_size = image_pre_proc.shape)\n",
    "                resized_image = tf.image.resize(trans_image, [image_size, image_size])[:, :, 0]\n",
    "                resized_image = keras.applications.xception.preprocess_input(resized_image*255)\n",
    "            else:\n",
    "                resized_image = tf.image.resize(image_pre_proc, [image_size, image_size])[:, :, 0]\n",
    "                resized_image = keras.applications.xception.preprocess_input(resized_image*255)\n",
    "                \n",
    "            out_img = tf.stack([resized_image, resized_image, resized_image], axis=-1)\n",
    "            imgs_master.append(out_img)\n",
    "        \n",
    "#         imgs_master_cutup_mixup = cutmix_mixup(imgs_master)\n",
    "        imgs_master_out = tf.stack(imgs_master)\n",
    "\n",
    "        # target column\n",
    "        FVC = self.dataset.loc[ids_tmp, 'FVC'].to_numpy(dtype=np.float32)\n",
    "\n",
    "        # to tensor convert\n",
    "        FVC = tf.convert_to_tensor(tf.constant(FVC))\n",
    "        tab = tf.convert_to_tensor(tf.constant(tab))\n",
    "        \n",
    "        return [imgs_master_out, tab], FVC\n",
    " \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(self.dataset.shape[0])\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "#         if self.shuffle:\n",
    "#             np.random.shuffle(self.indexes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Image_Input (InputLayer)        [(None, 299, 299, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "xception (Functional)           (None, 10, 10, 2048) 20861480    Image_Input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "pretrained_global_avg_pooling ( (None, 2048)         0           xception[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "output_dense_img_1 (Dense)      (None, 64)           131136      pretrained_global_avg_pooling[0][\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64)           256         output_dense_img_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_img_1 (Dropout)         (None, 64)           0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "output_dense_img_2 (Dense)      (None, 16)           1040        dropout_img_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concat (Concatenate)            (None, 24)           0           output_dense_img_2[0][0]         \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concat_out_2 (Dense)            (None, 64)           1600        concat[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 64)           256         concat_out_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_img_tab_2 (Dropout)     (None, 64)           0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concat_out_3 (Dense)            (None, 32)           2080        dropout_img_tab_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "p1 (Dense)                      (None, 3)            99          concat_out_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "p2 (Dense)                      (None, 3)            99          concat_out_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "preds (Lambda)                  (None, 3)            0           p1[0][0]                         \n",
      "                                                                 p2[0][0]                         \n",
      "==================================================================================================\n",
      "Total params: 20,998,046\n",
      "Trainable params: 136,310\n",
      "Non-trainable params: 20,861,736\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      " N Layers of BASE model --> 132\n",
      " N Layers of our model --> 16\n",
      "0 Image_Input\n",
      "1 xception\n",
      "2 pretrained_global_avg_pooling\n",
      "3 output_dense_img_1\n",
      "4 batch_normalization_4\n",
      "5 dropout_img_1\n",
      "6 output_dense_img_2\n",
      "7 input_2\n",
      "8 concat\n",
      "9 concat_out_2\n",
      "10 batch_normalization_5\n",
      "11 dropout_img_tab_2\n",
      "12 concat_out_3\n",
      "13 p1\n",
      "14 p2\n",
      "15 preds\n"
     ]
    }
   ],
   "source": [
    "# def get_dropout(input_tensor, p=0.5, mc=False):\n",
    "#     if mc:\n",
    "#         return keras.layers.Dropout(p)(input_tensor, training=True)\n",
    "#     else:\n",
    "#         return keras.layers.Dropout(p)(input_tensor)\n",
    "\n",
    "\n",
    "# def get_model():\n",
    "\n",
    "# with tpu_strategy.scope():\n",
    "input_shape_image=(image_size,image_size,3)\n",
    "input_image = keras.layers.Input(shape=input_shape_image, name=\"Image_Input\")\n",
    "\n",
    "########## Im,age Input ##########\n",
    "# base_model = keras.applications.VGG19(weights=\"imagenet\", include_top=False,\n",
    "#             input_tensor=keras.layers.Input(shape=input_shape_image))\n",
    "base_model = keras.applications.Xception(weights='imagenet', include_top=False, input_shape=input_shape_image)\n",
    "base_model.trainable=False\n",
    "base_model_output = base_model(input_image, training=False)\n",
    "layer_global_pooling = keras.layers.GlobalAveragePooling2D(name=\"pretrained_global_avg_pooling\")(base_model_output)\n",
    "out_dense_1 = keras.layers.Dense(64, name =\"output_dense_img_1\",activation=\"relu\")(layer_global_pooling)  \n",
    "bn_out_1 = keras.layers.BatchNormalization()(out_dense_1)\n",
    "\n",
    "\n",
    "dropout_img_1 = keras.layers.Dropout(0.45, name=\"dropout_img_1\")(bn_out_1)\n",
    "# Monte Carlo Dropout\n",
    "\n",
    "#     dropout_img_1 = get_dropout(bn_out_1, p=0.45, mc=mc)\n",
    "\n",
    "out_dense_2 = keras.layers.Dense(16, name =\"output_dense_img_2\",activation=\"relu\")(dropout_img_1)  \n",
    "# out_image = keras.layers.Dense(16, name =\"output_image\",activation=\"relu\")(dropout_img_2)  \n",
    "\n",
    "########## Tabular Input ##########\n",
    "tabular_inp= keras.layers.Input(shape=(8,))\n",
    "# Concat - Image and Tabular\n",
    "concat = keras.layers.Concatenate(name=\"concat\")([out_dense_2, tabular_inp])\n",
    "concat_out_1 = keras.layers.Dense(64, name=\"concat_out_2\", activation=\"relu\",\n",
    "                                 kernel_regularizer=keras.regularizers.l2(0.1)\n",
    "                                 )(concat)\n",
    "bn_out_2 = keras.layers.BatchNormalization()(concat_out_1)\n",
    "drop_out_img_tab_1 = keras.layers.Dropout(0.25, name=\"dropout_img_tab_2\")(bn_out_2)\n",
    "#     drop_out_img_tab_1 = get_dropout(bn_out_2, p=0.2, mc=mc)  \n",
    "concat_out_2 = keras.layers.Dense(32, name=\"concat_out_3\", activation=\"relu\")(drop_out_img_tab_1)\n",
    "p1 = keras.layers.Dense(3, activation=\"linear\", name=\"p1\")(concat_out_2)\n",
    "p2 = keras.layers.Dense(3, activation=\"relu\", name=\"p2\")(concat_out_2)\n",
    "preds = keras.layers.Lambda(lambda x: x[0] + tf.cumsum(x[1], axis=1), \n",
    "                 name=\"preds\")([p1, p2])\n",
    "\n",
    "model = keras.Model(inputs = [input_image, tabular_inp], outputs = preds)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "print(model.summary())\n",
    "print(f\" N Layers of BASE model --> {len(base_model.layers)}\")\n",
    "print(f\" N Layers of our model --> {len(model.layers)}\")\n",
    "for index, layer in enumerate(model.layers):\n",
    "    print(index, layer.name)\n",
    "\n",
    "#     return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN MODEL FOR INPUT FOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N Folds in Cross Validation --> 4\n",
      "############ Before dropping Junk Patients ################\n",
      "Train Shape --> (1113, 11)\n",
      "Number of unique patients in Train --> 127\n",
      "Validation Shape --> (369, 11)\n",
      "Number of unique patients in Validation --> 42\n",
      "############ After dropping Junk Patients ################\n",
      "Train Shape --> (1096, 11)\n",
      "Number of unique patients in Train --> 125\n",
      "Validation Shape --> (369, 11)\n",
      "Number of unique patients in Validation --> 42\n",
      "Epoch 1/100\n",
      "68/68 [==============================] - 52s 766ms/step - loss: 1086.0342 - score: 24.7801 - val_loss: 1058.7245 - val_score: 24.7981\n",
      "Epoch 2/100\n",
      "68/68 [==============================] - 52s 760ms/step - loss: 1055.3392 - score: 24.4163 - val_loss: 962.6561 - val_score: 18.4935\n",
      "Epoch 3/100\n",
      "68/68 [==============================] - 50s 742ms/step - loss: 934.8427 - score: 15.5604 - val_loss: 814.6988 - val_score: 11.7471\n",
      "Epoch 4/100\n",
      "68/68 [==============================] - 51s 755ms/step - loss: 691.2621 - score: 10.5226 - val_loss: 684.6096 - val_score: 11.1643\n",
      "Epoch 5/100\n",
      "68/68 [==============================] - 52s 757ms/step - loss: 393.2577 - score: 8.6776 - val_loss: 434.4353 - val_score: 9.1966\n",
      "Epoch 6/100\n",
      "68/68 [==============================] - 50s 736ms/step - loss: 242.7458 - score: 8.1843 - val_loss: 405.8531 - val_score: 9.0677\n",
      "Epoch 7/100\n",
      "68/68 [==============================] - 50s 735ms/step - loss: 221.2693 - score: 8.1721 - val_loss: 320.5120 - val_score: 8.4246\n",
      "Epoch 8/100\n",
      "68/68 [==============================] - 51s 757ms/step - loss: 218.7622 - score: 8.2307 - val_loss: 241.9245 - val_score: 8.1666\n",
      "Epoch 9/100\n",
      "68/68 [==============================] - 52s 762ms/step - loss: 196.8943 - score: 8.0948 - val_loss: 189.0598 - val_score: 7.9793\n",
      "Epoch 10/100\n",
      "68/68 [==============================] - 51s 753ms/step - loss: 182.2365 - score: 8.0588 - val_loss: 136.8262 - val_score: 7.8155\n",
      "Epoch 11/100\n",
      "68/68 [==============================] - 50s 738ms/step - loss: 185.4619 - score: 8.0458 - val_loss: 156.5576 - val_score: 7.8778\n",
      "Epoch 12/100\n",
      "68/68 [==============================] - 50s 729ms/step - loss: 181.7360 - score: 8.0408 - val_loss: 143.4900 - val_score: 7.7679\n",
      "Epoch 13/100\n",
      "68/68 [==============================] - 50s 736ms/step - loss: 179.5565 - score: 8.0442 - val_loss: 130.4684 - val_score: 7.7755\n",
      "Epoch 14/100\n",
      "68/68 [==============================] - 50s 738ms/step - loss: 170.8242 - score: 7.9718 - val_loss: 127.0571 - val_score: 7.7161\n",
      "Epoch 15/100\n",
      "68/68 [==============================] - 49s 727ms/step - loss: 180.7226 - score: 8.0530 - val_loss: 124.1639 - val_score: 7.6958\n",
      "Epoch 16/100\n",
      "68/68 [==============================] - 50s 737ms/step - loss: 176.8351 - score: 7.9998 - val_loss: 123.4100 - val_score: 7.7714\n",
      "Epoch 17/100\n",
      "68/68 [==============================] - 50s 732ms/step - loss: 161.2571 - score: 7.9248 - val_loss: 116.9625 - val_score: 7.6982\n",
      "Epoch 18/100\n",
      "68/68 [==============================] - 50s 735ms/step - loss: 158.4522 - score: 7.9007 - val_loss: 117.1823 - val_score: 7.6788\n",
      "Epoch 19/100\n",
      "68/68 [==============================] - 50s 730ms/step - loss: 159.9906 - score: 7.8984 - val_loss: 113.8625 - val_score: 7.6582\n",
      "Epoch 20/100\n",
      "68/68 [==============================] - 50s 738ms/step - loss: 173.8611 - score: 8.0198 - val_loss: 111.9855 - val_score: 7.6589\n",
      "Epoch 21/100\n",
      "68/68 [==============================] - 49s 718ms/step - loss: 174.0708 - score: 8.0067 - val_loss: 124.2458 - val_score: 7.6894\n",
      "Epoch 22/100\n",
      "68/68 [==============================] - 49s 715ms/step - loss: 158.7826 - score: 7.9107 - val_loss: 112.0878 - val_score: 7.6725\n",
      "Epoch 23/100\n",
      "68/68 [==============================] - 49s 722ms/step - loss: 163.3824 - score: 7.9204 - val_loss: 104.2309 - val_score: 7.6134\n",
      "Epoch 24/100\n",
      "47/68 [===================>..........] - ETA: 12s - loss: 167.0035 - score: 7.9523"
     ]
    }
   ],
   "source": [
    "# def run(model, fold):\n",
    "\n",
    "# read folds data and get train & validation data \n",
    "# model = get_model()\n",
    "df_folds = pd.read_csv(f\"{OUTPUT_PATH}/osic_{n_folds}_folds.csv\")\n",
    "print(f\"N Folds in Cross Validation --> {df_folds.fold.nunique()}\")\n",
    "df_train = df_folds[df_folds.fold !=fold].reset_index(drop=True)\n",
    "df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "print(\"############ Before dropping Junk Patients ################\")\n",
    "print(f\"Train Shape --> {df_train.shape}\")\n",
    "print(f\"Number of unique patients in Train --> {df_train.Patient.nunique()}\")\n",
    "# get validation data\n",
    "df_valid = df_folds[df_folds.fold==fold].reset_index(drop=True)  \n",
    "print(f\"Validation Shape --> {df_valid.shape}\")\n",
    "print(f\"Number of unique patients in Validation --> {df_valid.Patient.nunique()}\")\n",
    "\n",
    "\n",
    "print(\"############ After dropping Junk Patients ################\")\n",
    "\n",
    "\n",
    "df_train = df_train[~df_train.Patient.isin(junk_patients)].reset_index(drop=True)\n",
    "df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "df_valid = df_valid[~df_valid.Patient.isin(junk_patients)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train Shape --> {df_train.shape}\")\n",
    "print(f\"Number of unique patients in Train --> {df_train.Patient.nunique()}\")\n",
    "print(f\"Validation Shape --> {df_valid.shape}\")\n",
    "print(f\"Number of unique patients in Validation --> {df_valid.Patient.nunique()}\")\n",
    "\n",
    "\n",
    "# compile model\n",
    "# lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=1)\n",
    "# optimizer = keras.optimizers.Adam(lr=0.03)\n",
    "# optimizer = keras.optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999)\n",
    "# optimizer = keras.optimizers.SGD(lr=0.05, momentum=0.9)\n",
    "# optimizer = keras.optimizers.Nadam(lr=0.003, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "\n",
    "model.compile(loss=mloss(0.8), \n",
    "          optimizer=optimizer, metrics=[score])\n",
    "\n",
    "# check point and early stopping\n",
    "\n",
    "# fit model \n",
    "history = model.fit(DataGenerator(dataset=df_train, batch_size=bs, aug=True, shuffle=True),\n",
    "                    validation_data=DataGenerator(dataset=df_valid, batch_size=bs, aug=False, shuffle=False), \n",
    "                    epochs=100, callbacks=[checkpoint_cb, early_stopping_cb, lr_scheduler])\n",
    "\n",
    "# print validation data score\n",
    "print(f\"Validation Data Score --> {model.evaluate(DataGenerator(dataset=df_valid, batch_size=bs, aug=False, shuffle=False))}\")\n",
    "\n",
    "# return model via UDF\n",
    "# return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Validation Predictions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_valid_tmp = df_valid[['Patient', 'week', 'FVC', 'fold']]\n",
    "# df_valid_tmp['confidence'] = valid_preds[:, 2] - valid_preds[:, 0]\n",
    "# df_valid_tmp['pred_fvc'] = valid_preds[:, 1]\n",
    "# df_valid_tmp['sigma_clipped'] = df_valid_tmp['confidence'].apply(lambda x: max(x, 70))\n",
    "# df_valid_tmp['diff'] = abs(df_valid_tmp['FVC'] - df_valid_tmp['pred_fvc'])\n",
    "# df_valid_tmp['delta'] = df_valid_tmp['diff'].apply(lambda x: min(x, 1000))\n",
    "# df_valid_tmp['score'] = -math.sqrt(2)*df_valid_tmp['delta']/df_valid_tmp['sigma_clipped'] - np.log(math.sqrt(2)*df_valid_tmp['sigma_clipped'])\n",
    "# df_valid_tmp.score.mean()\n",
    "valid_preds = model.predict(DataGenerator(dataset=df_valid, batch_size=4, aug=True, shuffle=False))\n",
    "print(valid_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Test Predictions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(f\"{OUTPUT_PATH}/osic_test.csv\")\n",
    "print(df_test.shape)\n",
    "df_test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Results WITHOUT Augmentation on BASE FREEZED + CUSTOM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(DataGenerator(dataset=df_test, batch_size=4, aug=False, shuffle=False))\n",
    "df_test['confidence'] = test_pred[:, 2] - test_pred[:, 0]\n",
    "df_test['pred_fvc'] = test_pred[:, 1]\n",
    "df_test['sigma_clipped'] = df_test['confidence'].apply(lambda x: max(x, 70))\n",
    "df_test['diff'] = abs(df_test['FVC'] - df_test['pred_fvc'])\n",
    "df_test['delta'] = df_test['diff'].apply(lambda x: min(x, 1000))\n",
    "df_test['score'] = -math.sqrt(2)*df_test['delta']/df_test['sigma_clipped'] - np.log(math.sqrt(2)*df_test['sigma_clipped'])\n",
    "\n",
    "print(f\"# Test Data shape --> {df_test.shape}\")\n",
    "print(f\"# Predictions shape --> {test_pred.shape}\")\n",
    "print(f\"\\n**Overall Mean Score WITHOUT Augmentation on BASE FREEZED + CUSTOM MODEL --> {np.mean(df_test.score)} *****\")\n",
    "print(f\"\\n**Patient level Mean score -->\\n{df_test.groupby('Patient')['score'].mean()}\")\n",
    "df_test.sample(frac=1).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test results WITH Augmentation on BASE FREEZED + CUSTOM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_aug = np.stack([model.predict(DataGenerator(dataset=df_test, batch_size=4, aug=True, shuffle=False)) for sample in range(TTA_STEPS)])\n",
    "test_pred_mean = test_pred_aug.mean(axis=0)\n",
    "df_test['confidence'] = test_pred_mean[:, 2] - test_pred_mean[:, 0]\n",
    "df_test['pred_fvc'] = test_pred_mean[:, 1]\n",
    "df_test['sigma_clipped'] = df_test['confidence'].apply(lambda x: max(x, 70))\n",
    "df_test['diff'] = abs(df_test['FVC'] - df_test['pred_fvc'])\n",
    "df_test['delta'] = df_test['diff'].apply(lambda x: min(x, 1000))\n",
    "df_test['score'] = -math.sqrt(2)*df_test['delta']/df_test['sigma_clipped'] - np.log(math.sqrt(2)*df_test['sigma_clipped'])\n",
    "\n",
    "print(f\"# Test Data shape --> {df_test.shape}\")\n",
    "print(f\"# Predictions shape --> {test_pred_aug.shape}\")\n",
    "print(f\"\\n**Overall Mean Score WITH Augmentation on BASE FREEZED + CUSTOM MODEL --> {np.mean(df_test.score)} *****\")\n",
    "print(f\"\\n**Patient level Mean score -->\\n{df_test.groupby('Patient')['score'].mean()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINE TUNING BASE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable=True\n",
    "print(f\"N layers in base model --> {len(base_model.layers)}\")\n",
    "print(f\"Freezing Top {trainable} layers\")\n",
    "for layer in base_model.layers[:(len(base_model.layers) - trainable)]:\n",
    "        layer.trainable = False\n",
    "for layer in base_model.layers[(len(base_model.layers) - trainable):]:\n",
    "    layer.trainable = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=early_stopping_fine_tuning, restore_best_weights=True)\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=(0.00125/10), momentum=0.9, nesterov=True)\n",
    "model.compile(loss=mloss(0.8), optimizer=optimizer, metrics=[score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(DataGenerator(dataset=df_train, batch_size=bs, aug=True, shuffle=True),\n",
    "                    validation_data=DataGenerator(dataset=df_valid, batch_size=bs, aug=False, shuffle=False), \n",
    "                    epochs=100, callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "\n",
    "print(f\"Validation Data Score --> {model.evaluate(DataGenerator(dataset=df_valid, batch_size=bs, aug=False, shuffle=False))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Results WITHOUT Augmentation on FINE TUNED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(DataGenerator(dataset=df_test, batch_size=4, aug=False, shuffle=False))\n",
    "df_test['confidence'] = test_pred[:, 2] - test_pred[:, 0]\n",
    "df_test['pred_fvc'] = test_pred[:, 1]\n",
    "df_test['sigma_clipped'] = df_test['confidence'].apply(lambda x: max(x, 70))\n",
    "df_test['diff'] = abs(df_test['FVC'] - df_test['pred_fvc'])\n",
    "df_test['delta'] = df_test['diff'].apply(lambda x: min(x, 1000))\n",
    "df_test['score'] = -math.sqrt(2)*df_test['delta']/df_test['sigma_clipped'] - np.log(math.sqrt(2)*df_test['sigma_clipped'])\n",
    "\n",
    "print(f\"# Test Data shape --> {df_test.shape}\")\n",
    "print(f\"# Predictions shape --> {test_pred.shape}\")\n",
    "print(f\"\\n**Overall Mean Score WITHOUT Augmentation ON FINE TUNED MODEL--> {np.mean(df_test.score)} *****\")\n",
    "print(f\"\\n**Patient level Mean score -->\\n{df_test.groupby('Patient')['score'].mean()}\")\n",
    "df_test.sample(frac=1).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Results WITH Augmentation on FINE TUNED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TTA_STEPS = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_aug = np.stack([model.predict(DataGenerator(dataset=df_test, batch_size=4, aug=True, shuffle=False)) for sample in range(TTA_STEPS)])\n",
    "test_pred_mean = test_pred_aug.mean(axis=0)\n",
    "df_test['confidence'] = test_pred_mean[:, 2] - test_pred_mean[:, 0]\n",
    "df_test['pred_fvc'] = test_pred_mean[:, 1]\n",
    "df_test['sigma_clipped'] = df_test['confidence'].apply(lambda x: max(x, 70))\n",
    "df_test['diff'] = abs(df_test['FVC'] - df_test['pred_fvc'])\n",
    "df_test['delta'] = df_test['diff'].apply(lambda x: min(x, 1000))\n",
    "df_test['score'] = -math.sqrt(2)*df_test['delta']/df_test['sigma_clipped'] - np.log(math.sqrt(2)*df_test['sigma_clipped'])\n",
    "\n",
    "print(f\"# Test Data shape --> {df_test.shape}\")\n",
    "print(f\"# Predictions shape --> {test_pred_aug.shape}\")\n",
    "print(f\"\\n**Overall Mean Score WITH Augmentation ON FINE TUNED MODEL--> {np.mean(df_test.score)} *****\")\n",
    "print(f\"\\n**Patient level Mean score -->\\n{df_test.groupby('Patient')['score'].mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(df_test.score))\n",
    "df_test.groupby('Patient')['score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -7.1704979101657065\n",
    "# Patient\n",
    "# ID00419637202311204720264   -7.184254\n",
    "# ID00421637202311550012437   -7.128723\n",
    "# ID00422637202311677017371   -6.614698\n",
    "# ID00423637202312137826377   -7.785128\n",
    "# ID00426637202313170790466   -7.077930\n",
    "# Name: score, dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test['fold'] = fold\n",
    "# df_test\n",
    "# out_filename = f\"/kaggle/working/pred_test_{n_folds}FOLDS_{EPOCHS}E_fold_{fold}.csv\"\n",
    "# print(out_filename)\n",
    "# df_test.to_csv(out_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights('tmp_model_fold_0.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLAYGROUND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model2 = tf.keras.models.load_model(\"/kaggle/working/osic_pat_level_image_index_4FOLDS_100E_f_0.h5\", compile=False)\n",
    "# # print('model summary after loading in a scope')\n",
    "# # model2.summary()\n",
    "\n",
    "\n",
    "# Patient\n",
    "# ID00419637202311204720264   -8.151812\n",
    "# ID00421637202311550012437   -7.574516\n",
    "# ID00422637202311677017371   -7.408638\n",
    "# ID00423637202312137826377   -7.992481\n",
    "# ID00426637202313170790466   -7.528189\n",
    "# Name: score, dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = model.predict(DataGenerator(dataset=df_sub, batch_size=10, aug=False, shuffle=False))\n",
    "# print(a.shape)\n",
    "# df_sub['confidence'] = a[:, 2] - a[:, 0]\n",
    "# df_sub['pred_fvc'] = a[:, 1]\n",
    "# df_sub = df_sub.drop('FVC', axis=1)\n",
    "# out = pd.merge(df_sub, sub_mapping, on = [\"Patient\", \"week\"], how=\"inner\")\n",
    "# out[['Patient', 'Weeks', ]]\n",
    "# df_sub = pd.read_csv(f\"{OUTPUT_PATH}/osic_submission.csv\")\n",
    "# print(df_sub.shape)\n",
    "# sub_mapping = pd.read_csv(f\"{PATH_SUB}/osic_submission_mapping.csv\")\n",
    "# print(sub_mapping.shape)\n",
    "# df_sub.head()\n",
    "# preds_test_aug = np.stack([model2.predict(DataGenerator(dataset=df_test, batch_size=2, aug=True, shuffle=False)) for sample in range(25)])\n",
    "# preds_test_mean = preds_test_aug.mean(axis=0)\n",
    "# a = preds_test_mean\n",
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_preds_mc_dropout = np.stack([model(DataGenerator(dataset=df_test, batch_size=4, aug=True, shuffle=False), training=True) for sample in range(10)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test_preds_mc_dropout.shape\n",
    "# model(DataGenerator(dataset=df_test, batch_size=4, aug=True, shuffle=False), training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
