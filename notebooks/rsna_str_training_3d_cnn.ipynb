{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install monai\n",
    "# !pip install -q git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git\n",
    "# !pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0,1' # specify GPUs locally\n",
    "\n",
    "# libraries\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import PIL.Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm as tqdm\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, RandomSampler, SequentialSampler\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import albumentations\n",
    "\n",
    "import monai\n",
    "from monai.data import NiftiDataset\n",
    "from monai.transforms import AddChannel, Compose, RandRotate90, Resize, ScaleIntensity, ToTensor\n",
    "\n",
    "# from apex import amp # I cannot install apex in Kagggle notebook\n",
    "\n",
    "# device = torch.device('cuda')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)  \n",
    "    torch.cuda.manual_seed(seed)  \n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True # orig\n",
    "# DEBUG = False\n",
    "\n",
    "use_amp = False # orig\n",
    "# use_amp = True\n",
    "\n",
    "kernel_type = 'monai3d_160_3ch_1e-5_20ep_aug'\n",
    "\n",
    "image_size = 160\n",
    "data_dir = '/Users/paritoshgupta/Desktop/aamlp/aamlp/input/rnsa_str_pulmonary_embolism/train-jpegs'\n",
    "num_workers = 4\n",
    "init_lr = 5e-5\n",
    "out_dim = 9\n",
    "freeze_epo = 0\n",
    "warmup_epo = 1\n",
    "cosine_epo = 2 if DEBUG else 19\n",
    "n_epochs = freeze_epo + warmup_epo + cosine_epo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = [\n",
    "        'negative_exam_for_pe', # exam level\n",
    "        'rv_lv_ratio_gte_1', # exam level\n",
    "        'rv_lv_ratio_lt_1', # exam level\n",
    "        'leftsided_pe', # exam level\n",
    "        'chronic_pe', # exam level\n",
    "        'rightsided_pe', # exam level\n",
    "        'acute_and_chronic_pe', # exam level\n",
    "        'central_pe', # exam level\n",
    "        'indeterminate' # exam level\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/paritoshgupta/Desktop/aamlp/aamlp/input/rnsa_str_pulmonary_embolism/rsna_folds.csv\")\n",
    "df_study = df.drop_duplicates('StudyInstanceUID')[['StudyInstanceUID','SeriesInstanceUID','fold']+target_cols]\n",
    "print(df_study.shape)\n",
    "df_study.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    df_study = df_study.head(600)\n",
    "df_study.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from monai.transforms import LoadNifti, Randomizable, apply_transform\n",
    "from monai.transforms import AddChannel, Compose, RandRotate90, Resize, ScaleIntensity, ToTensor, RandAffine\n",
    "from monai.utils import get_seed\n",
    "\n",
    "class RSNADataset3D(torch.utils.data.Dataset, Randomizable):\n",
    "    def __init__(self, csv, mode, transform=None):\n",
    "\n",
    "        self.csv = csv.reset_index()\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.csv.shape[0]\n",
    "    \n",
    "    def randomize(self) -> None:\n",
    "        MAX_SEED = np.iinfo(np.uint32).max + 1\n",
    "        self._seed = self.R.randint(MAX_SEED, dtype=\"uint32\")    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        self.randomize()\n",
    "        row = self.csv.iloc[index]\n",
    "        jpg_lst = sorted(glob(os.path.join(data_dir, row.StudyInstanceUID, row.SeriesInstanceUID, '*.jpg')))\n",
    "        img_lst = [cv2.imread(jpg)[:,:,::-1] for jpg in jpg_lst] \n",
    "        img = np.stack([image.astype(np.float32) for image in img_lst], axis=2).transpose(3,0,1,2)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            if isinstance(self.transform, Randomizable):\n",
    "                self.transform.set_random_state(seed=self._seed)\n",
    "            img = apply_transform(self.transform, img)\n",
    "\n",
    "        if self.mode == 'test':\n",
    "            return img\n",
    "        else:\n",
    "            return img, torch.tensor(row[target_cols]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = Compose([ScaleIntensity(), \n",
    "                            Resize((image_size, image_size, image_size)), \n",
    "                            RandAffine( \n",
    "                                      prob=0.5,\n",
    "                                      translate_range=(5, 5, 5),\n",
    "                                      rotate_range=(np.pi * 4, np.pi * 4, np.pi * 4),\n",
    "                                      scale_range=(0.15, 0.15, 0.15),\n",
    "                                      padding_mode='border'),\n",
    "                            ToTensor()])\n",
    "val_transforms = Compose([ScaleIntensity(), Resize((image_size, image_size, image_size)), ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_show = RSNADataset3D(df_study.head(5), 'train', transform=val_transforms)\n",
    "dataset_show_aug = RSNADataset3D(df_study.head(5), 'train', transform=train_transforms)\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20,5\n",
    "for i in range(5):\n",
    "    f, axarr = plt.subplots(1,6)\n",
    "    img, label = dataset_show[i]\n",
    "    for j in range(6):        \n",
    "        if j<=2: axarr[j].imshow(img.numpy().transpose(1,2,3,0).mean(axis=j))\n",
    "        elif j==3: axarr[j].imshow(img.numpy().transpose(1,2,3,0)[image_size//2,:,:])\n",
    "        elif j==4: axarr[j].imshow(img.numpy().transpose(1,2,3,0)[:,image_size//2,:])\n",
    "        elif j==5: axarr[j].imshow(img.numpy().transpose(1,2,3,0)[:,:,image_size//2])\n",
    "        axarr[j].set_title(f\"Orig {i}\")\n",
    "    f, axarr = plt.subplots(1,6)\n",
    "    img, label = dataset_show_aug[i]    \n",
    "    for j in range(6):        \n",
    "        if j<=2: axarr[j].imshow(img.numpy().transpose(1,2,3,0).mean(axis=j))\n",
    "        elif j==3: axarr[j].imshow(img.numpy().transpose(1,2,3,0)[image_size//2,:,:])\n",
    "        elif j==4: axarr[j].imshow(img.numpy().transpose(1,2,3,0)[:,image_size//2,:])\n",
    "        elif j==5: axarr[j].imshow(img.numpy().transpose(1,2,3,0)[:,:,image_size//2])\n",
    "        axarr[j].set_title(f\"Aug {i}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce = nn.BCEWithLogitsLoss()\n",
    "def criterion(logits, target): \n",
    "    loss = bce(logits.view(-1), target.view(-1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    bar = tqdm(loader)\n",
    "    for (data, target) in bar:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(data)       \n",
    "        loss = criterion(logits, target)\n",
    "\n",
    "        if not use_amp:\n",
    "            loss.backward()\n",
    "        else:\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_np = loss.detach().cpu().numpy()\n",
    "        train_loss.append(loss_np)\n",
    "        smooth_loss = sum(train_loss[-100:]) / min(len(train_loss), 100)\n",
    "        bar.set_description('loss: %.5f, smth: %.5f' % (loss_np, smooth_loss))\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def val_epoch(model, loader, is_ext=None, n_test=1, get_output=False):\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    LOGITS = []\n",
    "    TARGETS = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (data, target) in tqdm(loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            logits = model(data)\n",
    "            LOGITS.append(logits.detach().cpu())\n",
    "            TARGETS.append(target.detach().cpu())\n",
    "\n",
    "    val_loss = criterion(torch.cat(LOGITS), torch.cat(TARGETS)).numpy()\n",
    "    PROBS = torch.sigmoid(torch.cat(LOGITS)).numpy().squeeze()    \n",
    "    LOGITS = torch.cat(LOGITS).numpy()\n",
    "    TARGETS = torch.cat(TARGETS).numpy()\n",
    "    \n",
    "    if get_output:\n",
    "        return LOGITS, PROBS, TARGETS\n",
    "    else:\n",
    "        acc = (PROBS.round() == TARGETS).mean() * 100.\n",
    "        auc = roc_auc_score(TARGETS, LOGITS)\n",
    "        return float(val_loss), acc, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradualWarmupSchedulerV2(GradualWarmupScheduler):\n",
    "    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
    "        super(GradualWarmupSchedulerV2, self).__init__(optimizer, multiplier, total_epoch, after_scheduler)\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch > self.total_epoch:\n",
    "            if self.after_scheduler:\n",
    "                if not self.finished:\n",
    "                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "                    self.finished = True\n",
    "                return self.after_scheduler.get_lr()\n",
    "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "        if self.multiplier == 1.0:\n",
    "            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(fold):\n",
    "    df_train = df_study[(df_study['fold'] != fold)]\n",
    "    df_valid = df_study[(df_study['fold'] == fold)]\n",
    "\n",
    "    dataset_train = RSNADataset3D(df_train, 'train', transform=train_transforms)\n",
    "    dataset_valid = RSNADataset3D(df_valid, 'val', transform=val_transforms)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=8, sampler=RandomSampler(dataset_train), num_workers=num_workers)\n",
    "    valid_loader = torch.utils.data.DataLoader(dataset_valid, batch_size=8, num_workers=num_workers)\n",
    "\n",
    "    model = monai.networks.nets.densenet.densenet121(spatial_dims=3, in_channels=3, out_channels=out_dim).to(device)\n",
    "\n",
    "    val_loss_best = 1000\n",
    "    model_file = f'{kernel_type}_best_fold{fold}.pth'\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=init_lr)\n",
    "    if use_amp:\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
    "#     if len(os.environ['CUDA_VISIBLE_DEVICES'].split(',')) > 1:\n",
    "#         model = nn.DataParallel(model)         \n",
    "        \n",
    "    scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, cosine_epo)\n",
    "    scheduler_warmup = GradualWarmupSchedulerV2(optimizer, multiplier=10, total_epoch=warmup_epo, after_scheduler=scheduler_cosine)\n",
    "\n",
    "    print(len(dataset_train), len(dataset_valid))\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        print(time.ctime(), 'Epoch:', epoch)\n",
    "        scheduler_warmup.step(epoch-1)\n",
    "\n",
    "        train_loss = train_epoch(model, train_loader, optimizer)\n",
    "        val_loss, acc, auc = val_epoch(model, valid_loader)\n",
    "    \n",
    "        content = time.ctime() + ' ' + f'Fold {fold}, Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {np.mean(train_loss):.5f}, valid loss: {(val_loss):.5f}, acc: {(acc):.4f}, auc: {(auc):.6f}'\n",
    "        print(content)\n",
    "        with open(f'log_{kernel_type}.txt', 'a') as appender:\n",
    "            appender.write(content + '\\n')             \n",
    "            \n",
    "        if val_loss < val_loss_best:\n",
    "            print('val_loss_best ({:.6f} --> {:.6f}).  Saving model ...'.format(val_loss_best, val_loss))\n",
    "            torch.save(model.state_dict(), model_file)\n",
    "            val_loss_best = val_loss\n",
    "\n",
    "    torch.save(model.state_dict(), f'{kernel_type}_model_fold{fold}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(fold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
