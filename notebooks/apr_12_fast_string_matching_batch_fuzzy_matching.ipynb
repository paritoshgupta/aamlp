{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading packages\n",
    "import os\n",
    "import pandas as pd\n",
    "# !pip install sparse-dot-topn\n",
    "import sparse_dot_topn\n",
    "import sparse_dot_topn.sparse_dot_topn as ct\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.sparse import csr_matrix\n",
    "import time\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import multiprocessing\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fast string matching using tf-idf, ngrams and cosine \n",
    "current_dir = \"/Users/paritoshgupta/Downloads/\"\n",
    "input_file_name = \"apr_10_null_state_entries_clusters.csv\"\n",
    "name_col = \"LNAME_OR_CORPNAME\"\n",
    "primary_key = \"SUB_ENTITY_ID\"\n",
    "cols_to_filter = ['SUB_ENTITY_ID','LNAME_OR_CORPNAME']\n",
    "cosine_sim_lower_cutoff = 0.99\n",
    "batch_size = 250000\n",
    "output_seed_file_name = \"apr12_seed_data_null_states.csv\"\n",
    "\n",
    "## Fuzzy Wuzzy\n",
    "name_matching_col = \"LNAME_OR_CORPNAME\"\n",
    "lower_cutoff_name_matching = 95\n",
    "output_fuzzy_match_file_name = \"apr12_fuzzy_match_null_states.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(string, n=3):\n",
    "    string = re.sub(r'[,-./]|\\sBD',r'', string)\n",
    "    ngrams = zip(*[string[i:] for i in range(n)])\n",
    "    return [''.join(ngram) for ngram in ngrams]\n",
    "\n",
    "def awesome_cossim_top(A, B, ntop, lower_bound=0):\n",
    "    # force A and B as a CSR matrix.\n",
    "    # If they have already been CSR, there is no overhead\n",
    "    A = A.tocsr()\n",
    "    B = B.tocsr()\n",
    "    M, _ = A.shape\n",
    "    _, N = B.shape\n",
    " \n",
    "    idx_dtype = np.int32\n",
    " \n",
    "    nnz_max = M*ntop\n",
    " \n",
    "    indptr = np.zeros(M+1, dtype=idx_dtype)\n",
    "    indices = np.zeros(nnz_max, dtype=idx_dtype)\n",
    "    data = np.zeros(nnz_max, dtype=A.dtype)\n",
    "\n",
    "    ct.sparse_dot_topn(\n",
    "        M, N, np.asarray(A.indptr, dtype=idx_dtype),\n",
    "        np.asarray(A.indices, dtype=idx_dtype),\n",
    "        A.data,\n",
    "        np.asarray(B.indptr, dtype=idx_dtype),\n",
    "        np.asarray(B.indices, dtype=idx_dtype),\n",
    "        B.data,\n",
    "        ntop,\n",
    "        lower_bound,\n",
    "        indptr, indices, data)\n",
    "\n",
    "    return csr_matrix((data,indices,indptr),shape=(M,N))\n",
    "\n",
    "def get_matches_df(sparse_matrix, name_vector, top=100):\n",
    "    non_zeros = sparse_matrix.nonzero()\n",
    "    \n",
    "    sparserows = non_zeros[0]\n",
    "    sparsecols = non_zeros[1]\n",
    "    \n",
    "    if top:\n",
    "        nr_matches = top\n",
    "    else:\n",
    "        nr_matches = sparsecols.size\n",
    "    \n",
    "    left_side = np.empty([nr_matches], dtype=object)\n",
    "    right_side = np.empty([nr_matches], dtype=object)\n",
    "    similairity = np.zeros(nr_matches)\n",
    "    \n",
    "    for index in range(0, nr_matches):\n",
    "        left_side[index] = name_vector[sparserows[index]]\n",
    "        right_side[index] = name_vector[sparsecols[index]]\n",
    "        similairity[index] = sparse_matrix.data[index]\n",
    "    \n",
    "    return pd.DataFrame({'InteractorA': left_side,\n",
    "                          'InteractorB': right_side,\n",
    "                           'Cosine_Similairity': similairity})\n",
    "\n",
    "\n",
    "def fast_string_matching_batch(data,name_col,cossim_lower_cutoff, batch_size):\n",
    "    print(\"############## Starting Fast String Matching... ############## \")\n",
    "    start_func = time.time()\n",
    "    big_frame = pd.DataFrame()\n",
    "    data = data.sort_values(name_col,ascending=True).reset_index(drop=True)\n",
    "    print(\"Raw Input Data Size: \", data.shape)\n",
    "    print(\"\"); print(\"\")\n",
    "    n_rows = data.shape[0]\n",
    "    for i in range(0,n_rows,batch_size):\n",
    "        if i + batch_size <= n_rows:\n",
    "            print(\"# Fast String Matching on batch of data with row number - \" + \n",
    "                  str(i) + \" to \" + str(i + batch_size) +\"  #\")\n",
    "            df_sample = data[i:i+batch_size]\n",
    "        else:\n",
    "            df_sample = data[i:]  \n",
    "            print(\"# Fast String Matching on batch of data with row number - \" + str(i) + \" to \" + str(n_rows) +\n",
    "                             \"  #\")\n",
    "        company_names = df_sample[name_col].drop_duplicates().reset_index(drop=True)\n",
    "        print(\"Unique list size: \", str(len(company_names)))\n",
    "        start = time.time()\n",
    "        vectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams)\n",
    "        tf_idf_matrix = vectorizer.fit_transform(company_names)\n",
    "        matches = awesome_cossim_top(tf_idf_matrix, tf_idf_matrix.transpose(), 10000, cossim_lower_cutoff)\n",
    "        matches_df = get_matches_df(matches, company_names, top=100000)\n",
    "        # Remove all exact matches\n",
    "        matches_df = matches_df[matches_df['Cosine_Similairity'] < 0.99999].drop_duplicates().reset_index(drop=True) \n",
    "        matches_df = matches_df.sort_values('Cosine_Similairity',ascending=False).reset_index(drop=True)\n",
    "        print(\"Output shape: \", matches_df.shape)\n",
    "        if len(matches_df) > 0: \n",
    "            matches_df['check_string'] = matches_df.apply(lambda row: ''.join(sorted([row['InteractorA'], \n",
    "                                                                                      row['InteractorB']])), axis=1)\n",
    "            matches_df = matches_df.drop_duplicates('check_string').reset_index(drop=True)\n",
    "            matches_df = matches_df.drop(['check_string'],axis=1)\n",
    "            print(\"Output shape after dropping swapped duplicates: \", matches_df.shape)\n",
    "            matches_df = matches_df[~matches_df['InteractorB'].isin(matches_df['InteractorA'])]\n",
    "            big_frame = big_frame.append(matches_df, ignore_index=True)  \n",
    "            print(\"Execution time of Fast String Matching on current batch: \" + str(execution_time_parser(start)))\n",
    "            print(\"\"); print(\"\")\n",
    "    \n",
    "    print(\"### Total Execution Time: \" + str(execution_time_parser(start_func)) + \"###\")\n",
    "    \n",
    "    return big_frame\n",
    "        \n",
    "def melt_data_create_clusters(input_data,name_col):\n",
    "    input_data['CLUSTER_ID'] = pd.Categorical(input_data['InteractorA'].astype(str)).codes\n",
    "    melted_data = pd.melt(input_data, id_vars='CLUSTER_ID', \n",
    "                                 value_vars=['InteractorA','InteractorB'], \\\n",
    "                                 var_name=None, value_name=name_col, \\\n",
    "                                 col_level=None).drop('variable',axis=1).sort_values('CLUSTER_ID'). \\\n",
    "                                 reset_index(drop=True)[[name_col,'CLUSTER_ID']]\n",
    "    return melted_data\n",
    "    \n",
    "def execution_time_parser(start):\n",
    "    end = time.time()\n",
    "    temp = end - start\n",
    "    hours = temp // 3600\n",
    "    temp = temp - 3600 * hours\n",
    "    minutes = temp // 60\n",
    "    seconds = temp - 60 * minutes\n",
    "    time_taken = str(hours) + \" hours \" + str(minutes) + \" minutes \" + str(int(seconds)) + \" seconds\"\n",
    "\n",
    "    return time_taken\n",
    "\n",
    "def seed_delta_data_clean(data, type):\n",
    "    try:\n",
    "        if type == \"seed\":\n",
    "            data.columns = [\"SEED_\" + x for x in data.columns]\n",
    "        else:\n",
    "            data.columns = [\"DELTA_\" + x for x in data.columns]\n",
    "        cols_to_clean = data.select_dtypes(exclude=['int64', 'int', 'int32', 'float64', 'float']).columns\n",
    "\n",
    "        for col in cols_to_clean:\n",
    "            data[col] = data[col].map(lambda x: \" \".join(x.strip().upper() for x in str(x).split()))\n",
    "\n",
    "    except Exception as e:\n",
    "        error = \"Error in cleaning Seed and Delta data: \" + str(e)\n",
    "        print(error)\n",
    "        sys.exit(1)\n",
    "\n",
    "    return data\n",
    "\n",
    "def seed_delta_matching_col_clean(input_text):\n",
    "    try:\n",
    "        input_text_mod = ' '.join([str(x) for x in input_text.lower().strip().split()])\n",
    "    except Exception as e:\n",
    "        error = \"Error in cleaning matching columns of Seed and Delta: \" + str(e)\n",
    "        print(error)\n",
    "        sys.exit(1)\n",
    "    return input_text_mod\n",
    "\n",
    "def iterator_fuzzy_score(text, dict_name, score_cutoff):\n",
    "    try:\n",
    "        tmp_tuple = process.extractOne(text, dict_name, score_cutoff=score_cutoff,\n",
    "                                       scorer=fuzz.ratio)\n",
    "        if tmp_tuple:\n",
    "            tmp_tuple = tmp_tuple[1:]\n",
    "        else:\n",
    "            tmp_tuple = (0, np.NaN)\n",
    "    except Exception as e:\n",
    "        error = \"Error in fuzzy score calculator: \" + str(e)\n",
    "        print(error)\n",
    "        print(error)\n",
    "        sys.exit(1)\n",
    "    return tmp_tuple\n",
    "\n",
    "\n",
    "def fuzzy_score_calculator_name(delta_data, seed_data, matching_col, primary_key, state_to_process, lower_cut_off):\n",
    "    try:\n",
    "        print(\"START TIME: \" + str(pd.datetime.now()))\n",
    "        print(\"Starting Name Fuzzy Matching for leftover Entities after Address matching for state: \"\n",
    "                     + str(state_to_process))\n",
    "        print(\"SEED Data Size: \" + str(len(seed_data)))\n",
    "        print(\"DELTA Data Size: \" + str(len(delta_data)))\n",
    "        delta_list_name = [seed_delta_matching_col_clean(x) for x in\n",
    "                           delta_data[str(\"DELTA_\" + matching_col)].values.tolist()]\n",
    "        seed_dict_name = dict(zip(seed_data[str(\"SEED_\" + primary_key)], [seed_delta_matching_col_clean(x) for\n",
    "                                                                          x in\n",
    "                                                                          seed_data[str(\"SEED_\" + matching_col)]]))\n",
    "\n",
    "        num_cores = multiprocessing.cpu_count()\n",
    "        print(\"Processing Fuzzy Matching on: \" + str(num_cores) + \" cores\")\n",
    "        tuple_list_name = Parallel(n_jobs=-1, backend=\"multiprocessing\")(\n",
    "            delayed(iterator_fuzzy_score)(delta_name, seed_dict_name,\n",
    "                                          lower_cut_off) for delta_name in tqdm(delta_list_name))\n",
    "\n",
    "        df_name_matching = pd.DataFrame(tuple_list_name, columns=['FUZZY_SCORE', str(\"SEED_\" + primary_key)])\n",
    "        df_out = pd.concat([delta_data, df_name_matching], axis=1)\n",
    "        df_out = pd.merge(df_out, seed_data, on=str(\"SEED_\" + primary_key), how=\"inner\")\n",
    "        df_out = df_out.drop(df_out[(df_out[str('DELTA_' + matching_col)] == \"not provided\") |\n",
    "                                    (df_out[str('SEED_' + matching_col)] == \"not provided\")].index)\n",
    "        df_out = df_out[df_out['FUZZY_SCORE'] >= lower_cut_off]\n",
    "        df_out['FUZZY_MATCHING_TYPE'] = \"NAME\"\n",
    "        print(\"Finished fuzzy matching on Name for state: \" + str(state_to_process))\n",
    "    except Exception as e:\n",
    "        error = \"Error in fuzzy matching on Name for state: \" + str(state_to_process) + \" \" + str(e)\n",
    "        print(error)\n",
    "        sys.exit(1)\n",
    "\n",
    "    return df_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Reading and % distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input data:  (2179588, 2)\n",
      "Columns present are: ['SUB_ENTITY_ID', 'LNAME_OR_CORPNAME']\n",
      "\n",
      "% NULL in NAME: 0.0\n"
     ]
    }
   ],
   "source": [
    "os.chdir(current_dir)\n",
    "data = pd.read_csv(input_file_name,low_memory=False)[cols_to_filter]\n",
    "print(\"shape of input data: \", data.shape)\n",
    "print(\"Columns present are:\", list(data.columns))\n",
    "\n",
    "# Missing % in Name column\n",
    "print(\"\\n% NULL in NAME:\", \n",
    "           str(100 * (1 - (data[~data[name_col].isnull()].shape[0])/data.shape[0])))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Processing using TF-IDF, n-grams: sequences of N contiguous items (in this case characters), Cosine Similarity ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############## Starting Fast String Matching... ############## \n",
      "Raw Input Data Size:  (2179588, 2)\n",
      "\n",
      "\n",
      "# Fast String Matching on batch of data with row number - 0 to 250000  #\n",
      "Unique list size:  236890\n",
      "Output shape:  (118, 3)\n",
      "Output shape after dropping swapped duplicates:  (59, 3)\n",
      "Execution time of Fast String Matching on current batch: 0.0 hours 3.0 minutes 12 seconds\n",
      "\n",
      "\n",
      "# Fast String Matching on batch of data with row number - 250000 to 500000  #\n",
      "Unique list size:  231934\n",
      "Output shape:  (338, 3)\n",
      "Output shape after dropping swapped duplicates:  (169, 3)\n",
      "Execution time of Fast String Matching on current batch: 0.0 hours 3.0 minutes 11 seconds\n",
      "\n",
      "\n",
      "# Fast String Matching on batch of data with row number - 500000 to 750000  #\n",
      "Unique list size:  237163\n",
      "Output shape:  (208, 3)\n",
      "Output shape after dropping swapped duplicates:  (104, 3)\n",
      "Execution time of Fast String Matching on current batch: 0.0 hours 3.0 minutes 9 seconds\n",
      "\n",
      "\n",
      "# Fast String Matching on batch of data with row number - 750000 to 1000000  #\n",
      "Unique list size:  233905\n",
      "Output shape:  (306, 3)\n",
      "Output shape after dropping swapped duplicates:  (153, 3)\n",
      "Execution time of Fast String Matching on current batch: 0.0 hours 3.0 minutes 16 seconds\n",
      "\n",
      "\n",
      "# Fast String Matching on batch of data with row number - 1000000 to 1250000  #\n",
      "Unique list size:  236241\n",
      "Output shape:  (145, 3)\n",
      "Output shape after dropping swapped duplicates:  (74, 3)\n",
      "Execution time of Fast String Matching on current batch: 0.0 hours 3.0 minutes 1 seconds\n",
      "\n",
      "\n",
      "# Fast String Matching on batch of data with row number - 1250000 to 1500000  #\n",
      "Unique list size:  233871\n",
      "Output shape:  (268, 3)\n",
      "Output shape after dropping swapped duplicates:  (134, 3)\n",
      "Execution time of Fast String Matching on current batch: 0.0 hours 3.0 minutes 30 seconds\n",
      "\n",
      "\n",
      "# Fast String Matching on batch of data with row number - 1500000 to 1750000  #\n",
      "Unique list size:  233770\n",
      "Output shape:  (351, 3)\n",
      "Output shape after dropping swapped duplicates:  (177, 3)\n",
      "Execution time of Fast String Matching on current batch: 0.0 hours 3.0 minutes 23 seconds\n",
      "\n",
      "\n",
      "# Fast String Matching on batch of data with row number - 1750000 to 2000000  #\n",
      "Unique list size:  235722\n",
      "Output shape:  (308, 3)\n",
      "Output shape after dropping swapped duplicates:  (154, 3)\n",
      "Execution time of Fast String Matching on current batch: 0.0 hours 4.0 minutes 33 seconds\n",
      "\n",
      "\n",
      "# Fast String Matching on batch of data with row number - 2000000 to 2179588  #\n",
      "Unique list size:  167180\n",
      "Output shape:  (238, 3)\n",
      "Output shape after dropping swapped duplicates:  (119, 3)\n",
      "Execution time of Fast String Matching on current batch: 0.0 hours 1.0 minutes 35 seconds\n",
      "\n",
      "\n",
      "### Total Execution Time: 0.0 hours 29.0 minutes 0 seconds###\n"
     ]
    }
   ],
   "source": [
    "bigframe = fast_string_matching_batch(data, name_col=name_col, \n",
    "                                      cossim_lower_cutoff=cosine_sim_lower_cutoff, batch_size=batch_size)\n",
    "# bigframe.to_csv(\"null_states_seed_data_raw.csv\",index=False)\n",
    "seed_clusters = melt_data_create_clusters(bigframe,name_col)\n",
    "seed_clusters.to_csv(output_seed_file_name,index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Creating seed and delta datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns present in SEED data:  ['SEED_LNAME_OR_CORPNAME', 'SEED_CLUSTER_ID', 'SEED_SUB_ENTITY_ID']\n",
      "Columns present in DELTA data:  ['DELTA_SUB_ENTITY_ID', 'DELTA_LNAME_OR_CORPNAME']\n"
     ]
    }
   ],
   "source": [
    "# seed_clusters = pd.read_csv(\"apr12_seed_data_null_states.csv\")\n",
    "seed_data = pd.merge(seed_clusters,data,on=name_col,how=\"inner\").reset_index(drop=True)\n",
    "delta_data = data[~data[primary_key].isin(seed_data[primary_key])].reset_index(drop=True)\n",
    "seed_data_clean = seed_delta_data_clean(data=seed_data, type=\"seed\")\n",
    "delta_data_clean = seed_delta_data_clean(data=delta_data, type=\"delta\") \n",
    "print(\"Columns present in SEED data: \", list(seed_data_clean.columns))\n",
    "print(\"Columns present in DELTA data: \", list(delta_data_clean.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Fuzzy Name Matching on NAME column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TIME: 2019-04-12 15:21:52.594052\n",
      "Starting Name Fuzzy Matching for leftover Entities after Address matching for state: NULL STATES\n",
      "SEED Data Size: 2930\n",
      "DELTA Data Size: 2176711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2176711 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Fuzzy Matching on: 12 cores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 555442/2176711 [55:10<2:33:43, 175.78it/s]WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: ',']\n",
      " 63%|██████▎   | 1372748/2176711 [2:28:58<1:55:09, 116.36it/s]WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '-']\n",
      " 73%|███████▎  | 1590764/2176711 [2:54:36<1:06:02, 147.87it/s]WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '.']\n",
      " 76%|███████▋  | 1660750/2176711 [3:02:00<1:16:53, 111.83it/s]WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '---------------------------']\n",
      " 77%|███████▋  | 1670427/2176711 [3:03:09<45:36, 185.03it/s]  WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: ''']\n",
      " 78%|███████▊  | 1697283/2176711 [3:05:50<49:19, 161.99it/s]WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '`']\n",
      " 84%|████████▍ | 1825056/2176711 [3:19:45<34:09, 171.56it/s]  WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '+']\n",
      " 97%|█████████▋| 2101493/2176711 [3:46:11<06:31, 191.93it/s]WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '-']\n",
      "100%|██████████| 2176711/2176711 [3:53:27<00:00, 155.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fuzzy matching on Name for state: NULL STATES\n",
      "######## Summary ########\n",
      "Output file name:  apr12_fuzzy_match_null_states.csv\n",
      "Output data shape:  8248\n",
      "##Total Execution Time: 3.0 hours 53.0 minutes 32 seconds##\n"
     ]
    }
   ],
   "source": [
    "tik = time.time()\n",
    "# delta_data_clean = delta_data_clean[0:10000] (testing purpose)\n",
    "df_out_fuzzy_matching_name = fuzzy_score_calculator_name(delta_data=delta_data_clean,\n",
    "                                                             seed_data=seed_data_clean,\n",
    "                                                             matching_col=name_matching_col,\n",
    "                                                             primary_key=primary_key,\n",
    "                                                             state_to_process=\"NULL STATES\",\n",
    "                                                             lower_cut_off=lower_cutoff_name_matching)\n",
    "df_out_fuzzy_matching_name.to_csv(output_fuzzy_match_file_name,index=False)\n",
    "\n",
    "print(\"######## Summary ########\")\n",
    "print(\"Output file name: \", str(output_fuzzy_match_file_name))\n",
    "print(\"Output data shape: \", str(len(df_out_fuzzy_matching_name)))\n",
    "print(\"##Total Execution Time: \" +  str(execution_time_parser(tik)) + \"##\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
